{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VariationalGraphAutoencoder.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zjKIUL154AMo",
        "outputId": "9db35501-96b5-4723-8f39-cca187646275"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.11.0+cu113.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-1.11.0%2Bcu113/torch_scatter-2.0.9-cp37-cp37m-linux_x86_64.whl (7.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.9 MB 4.5 MB/s \n",
            "\u001b[?25hInstalling collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.0.9\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.11.0+cu113.html\n",
            "Collecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-1.11.0%2Bcu113/torch_sparse-0.6.13-cp37-cp37m-linux_x86_64.whl (3.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 4.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-sparse) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scipy->torch-sparse) (1.21.6)\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.6.13\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.11.0+cu113.html\n",
            "Collecting torch-cluster\n",
            "  Downloading https://data.pyg.org/whl/torch-1.11.0%2Bcu113/torch_cluster-1.6.0-cp37-cp37m-linux_x86_64.whl (2.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5 MB 4.4 MB/s \n",
            "\u001b[?25hInstalling collected packages: torch-cluster\n",
            "Successfully installed torch-cluster-1.6.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.11.0+cu113.html\n",
            "Collecting torch-spline-conv\n",
            "  Downloading https://data.pyg.org/whl/torch-1.11.0%2Bcu113/torch_spline_conv-1.2.1-cp37-cp37m-linux_x86_64.whl (750 kB)\n",
            "\u001b[K     |████████████████████████████████| 750 kB 4.4 MB/s \n",
            "\u001b[?25hInstalling collected packages: torch-spline-conv\n",
            "Successfully installed torch-spline-conv-1.2.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.0.4.tar.gz (407 kB)\n",
            "\u001b[K     |████████████████████████████████| 407 kB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (4.64.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.4.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.23.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (3.0.9)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->torch-geometric) (2.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-geometric) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-geometric) (2022.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->torch-geometric) (1.15.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2022.5.18.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (3.0.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (1.1.0)\n",
            "Building wheels for collected packages: torch-geometric\n",
            "  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-geometric: filename=torch_geometric-2.0.4-py3-none-any.whl size=616603 sha256=8d569ec8d7c5d88db4b1dfe505d5fd4007ec1442239943e3adcc962f4f8f4eb5\n",
            "  Stored in directory: /root/.cache/pip/wheels/18/a6/a4/ca18c3051fcead866fe7b85700ee2240d883562a1bc70ce421\n",
            "Successfully built torch-geometric\n",
            "Installing collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.0.4\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "def format_pytorch_version(version):\n",
        "  return version.split('+')[0]\n",
        "\n",
        "TORCH_version = torch.__version__\n",
        "TORCH = format_pytorch_version(TORCH_version)\n",
        "\n",
        "def format_cuda_version(version):\n",
        "  return 'cu' + version.replace('.', '')\n",
        "\n",
        "CUDA_version = torch.version.cuda\n",
        "CUDA = format_cuda_version(CUDA_version)\n",
        "\n",
        "!pip install torch-scatter     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-sparse      -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-cluster     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-geometric "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import numpy\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "fRoIz8Sn4PGn"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch_geometric \n",
        "from torch_geometric.datasets import Planetoid\n",
        "import torch_geometric.transforms as T\n",
        "from torch_geometric.nn import GCNConv, VGAE, GAE\n",
        "from torch_geometric.utils import train_test_split_edges \n",
        "from torch_geometric.transforms import RandomLinkSplit"
      ],
      "metadata": {
        "id": "jFqEz_j64fal"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = Planetoid(\"\\..\", \"CiteSeer\", transform=T.NormalizeFeatures())\n",
        "data = dataset[0]\n",
        "data.train_mask = data.val_mask = data.test_mask = data.y = None\n",
        "data = train_test_split_edges(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QHDNwB7K4s_W",
        "outputId": "04b32649-8ba3-4871-a79a-415700259679"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.x\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.tx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.allx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.y\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.ty\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.ally\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.graph\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.citeseer.test.index\n",
            "Processing...\n",
            "Done!\n",
            "/usr/local/lib/python3.7/dist-packages/torch_geometric/deprecation.py:12: UserWarning: 'train_test_split_edges' is deprecated, use 'transforms.RandomLinkSplit' instead\n",
            "  warnings.warn(out)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# x -> Feature matrix [nodes, number_of_node_features]"
      ],
      "metadata": {
        "id": "s8pX_3n4KW7U"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data\n",
        "#edge index = adjacency matrix, test_neg_edges -> edges in test set that are not in the graph, test_pos_edge_index -> edges in test set that are in graph, x -> feature matrix, y -> labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tPVJudKoOO5K",
        "outputId": "ec3deca8-8381-43ec-c675-443a14f5b09a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Data(x=[3327, 3703], val_pos_edge_index=[2, 227], test_pos_edge_index=[2, 455], train_pos_edge_index=[2, 7740], train_neg_adj_mask=[3327, 3327], val_neg_edge_index=[2, 227], test_neg_edge_index=[2, 455])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Define the encoder"
      ],
      "metadata": {
        "id": "OF-fA9_OM9n_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.nn import VGAE"
      ],
      "metadata": {
        "id": "J_45KBY_QjMW"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VariationalGCNEncoder(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(VariationalGCNEncoder, self).__init__()\n",
        "        self.conv1 = GCNConv(in_channels, 2 * out_channels, cached=True) # cached only for transductive learning\n",
        "        self.conv_mu = GCNConv(2 * out_channels, out_channels, cached=True)\n",
        "        self.conv_logstd = GCNConv(2 * out_channels, out_channels, cached=True)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index).relu()\n",
        "        return self.conv_mu(x, edge_index), self.conv_logstd(x, edge_index)"
      ],
      "metadata": {
        "id": "BE30iqTuLwQ8"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Parameters\n",
        "\n",
        "out_channels = 2\n",
        "num_features = dataset.num_features\n",
        "epochs = 300\n",
        "\n",
        "\n",
        "model = VGAE(VariationalGCNEncoder(num_features, out_channels))  # new line\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "x = data.x.to(device)\n",
        "train_pos_edge_index = data.train_pos_edge_index.to(device)\n",
        "\n",
        "\n",
        "#Initialize the optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 0.01)"
      ],
      "metadata": {
        "id": "zuUvkP68NvJL"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    z = model.encode(x, train_pos_edge_index)\n",
        "    loss = model.recon_loss(z, train_pos_edge_index)\n",
        "    \n",
        "    loss = loss + (1 / data.num_nodes) * model.kl_loss()  # new line\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return float(loss)\n",
        "\n",
        "\n",
        "def test(pos_edge_index, neg_edge_index):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        z = model.encode(x, train_pos_edge_index)\n",
        "    return model.test(z, pos_edge_index, neg_edge_index)"
      ],
      "metadata": {
        "id": "ZZLmM1bYRQoH"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.tensorboard import SummaryWriter"
      ],
      "metadata": {
        "id": "yiTdxOatRlcv"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "writer = SummaryWriter('runs/VGAE_experiment_'+'2d_100_epochs')\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    loss = train()\n",
        "    auc, ap = test(data.test_pos_edge_index, data.test_neg_edge_index)\n",
        "    print('Epoch: {:03d}, AUC: {:.4f}, AP: {:.4f}'.format(epoch, auc, ap))\n",
        "    \n",
        "    \n",
        "    writer.add_scalar('auc train',auc,epoch) # new line\n",
        "    writer.add_scalar('ap train',ap,epoch)   # new line"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dKgNlhb7RiBx",
        "outputId": "8fc2d518-287f-4fbe-b988-00c30c4d3419"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, AUC: 0.6098, AP: 0.6356\n",
            "Epoch: 002, AUC: 0.6217, AP: 0.6468\n",
            "Epoch: 003, AUC: 0.6251, AP: 0.6500\n",
            "Epoch: 004, AUC: 0.6268, AP: 0.6519\n",
            "Epoch: 005, AUC: 0.6273, AP: 0.6524\n",
            "Epoch: 006, AUC: 0.6275, AP: 0.6528\n",
            "Epoch: 007, AUC: 0.6276, AP: 0.6528\n",
            "Epoch: 008, AUC: 0.6276, AP: 0.6528\n",
            "Epoch: 009, AUC: 0.6275, AP: 0.6528\n",
            "Epoch: 010, AUC: 0.6274, AP: 0.6524\n",
            "Epoch: 011, AUC: 0.6275, AP: 0.6525\n",
            "Epoch: 012, AUC: 0.6270, AP: 0.6521\n",
            "Epoch: 013, AUC: 0.6267, AP: 0.6516\n",
            "Epoch: 014, AUC: 0.6264, AP: 0.6511\n",
            "Epoch: 015, AUC: 0.6259, AP: 0.6507\n",
            "Epoch: 016, AUC: 0.6253, AP: 0.6499\n",
            "Epoch: 017, AUC: 0.6249, AP: 0.6493\n",
            "Epoch: 018, AUC: 0.6246, AP: 0.6488\n",
            "Epoch: 019, AUC: 0.6239, AP: 0.6480\n",
            "Epoch: 020, AUC: 0.6229, AP: 0.6468\n",
            "Epoch: 021, AUC: 0.6214, AP: 0.6456\n",
            "Epoch: 022, AUC: 0.6201, AP: 0.6445\n",
            "Epoch: 023, AUC: 0.6188, AP: 0.6435\n",
            "Epoch: 024, AUC: 0.6178, AP: 0.6428\n",
            "Epoch: 025, AUC: 0.6168, AP: 0.6416\n",
            "Epoch: 026, AUC: 0.6163, AP: 0.6411\n",
            "Epoch: 027, AUC: 0.6156, AP: 0.6404\n",
            "Epoch: 028, AUC: 0.6153, AP: 0.6402\n",
            "Epoch: 029, AUC: 0.6151, AP: 0.6400\n",
            "Epoch: 030, AUC: 0.6156, AP: 0.6404\n",
            "Epoch: 031, AUC: 0.6159, AP: 0.6407\n",
            "Epoch: 032, AUC: 0.6163, AP: 0.6413\n",
            "Epoch: 033, AUC: 0.6172, AP: 0.6426\n",
            "Epoch: 034, AUC: 0.6183, AP: 0.6437\n",
            "Epoch: 035, AUC: 0.6199, AP: 0.6450\n",
            "Epoch: 036, AUC: 0.6214, AP: 0.6461\n",
            "Epoch: 037, AUC: 0.6230, AP: 0.6477\n",
            "Epoch: 038, AUC: 0.6247, AP: 0.6494\n",
            "Epoch: 039, AUC: 0.6260, AP: 0.6508\n",
            "Epoch: 040, AUC: 0.6270, AP: 0.6520\n",
            "Epoch: 041, AUC: 0.6278, AP: 0.6529\n",
            "Epoch: 042, AUC: 0.6285, AP: 0.6538\n",
            "Epoch: 043, AUC: 0.6292, AP: 0.6545\n",
            "Epoch: 044, AUC: 0.6297, AP: 0.6552\n",
            "Epoch: 045, AUC: 0.6303, AP: 0.6559\n",
            "Epoch: 046, AUC: 0.6307, AP: 0.6563\n",
            "Epoch: 047, AUC: 0.6314, AP: 0.6571\n",
            "Epoch: 048, AUC: 0.6318, AP: 0.6576\n",
            "Epoch: 049, AUC: 0.6323, AP: 0.6582\n",
            "Epoch: 050, AUC: 0.6328, AP: 0.6587\n",
            "Epoch: 051, AUC: 0.6333, AP: 0.6594\n",
            "Epoch: 052, AUC: 0.6338, AP: 0.6599\n",
            "Epoch: 053, AUC: 0.6343, AP: 0.6605\n",
            "Epoch: 054, AUC: 0.6347, AP: 0.6610\n",
            "Epoch: 055, AUC: 0.6351, AP: 0.6613\n",
            "Epoch: 056, AUC: 0.6357, AP: 0.6618\n",
            "Epoch: 057, AUC: 0.6360, AP: 0.6623\n",
            "Epoch: 058, AUC: 0.6364, AP: 0.6627\n",
            "Epoch: 059, AUC: 0.6367, AP: 0.6632\n",
            "Epoch: 060, AUC: 0.6371, AP: 0.6637\n",
            "Epoch: 061, AUC: 0.6376, AP: 0.6644\n",
            "Epoch: 062, AUC: 0.6378, AP: 0.6647\n",
            "Epoch: 063, AUC: 0.6380, AP: 0.6652\n",
            "Epoch: 064, AUC: 0.6384, AP: 0.6658\n",
            "Epoch: 065, AUC: 0.6386, AP: 0.6662\n",
            "Epoch: 066, AUC: 0.6388, AP: 0.6669\n",
            "Epoch: 067, AUC: 0.6390, AP: 0.6674\n",
            "Epoch: 068, AUC: 0.6396, AP: 0.6682\n",
            "Epoch: 069, AUC: 0.6399, AP: 0.6689\n",
            "Epoch: 070, AUC: 0.6401, AP: 0.6694\n",
            "Epoch: 071, AUC: 0.6403, AP: 0.6699\n",
            "Epoch: 072, AUC: 0.6406, AP: 0.6704\n",
            "Epoch: 073, AUC: 0.6407, AP: 0.6708\n",
            "Epoch: 074, AUC: 0.6409, AP: 0.6718\n",
            "Epoch: 075, AUC: 0.6412, AP: 0.6724\n",
            "Epoch: 076, AUC: 0.6415, AP: 0.6730\n",
            "Epoch: 077, AUC: 0.6415, AP: 0.6735\n",
            "Epoch: 078, AUC: 0.6414, AP: 0.6740\n",
            "Epoch: 079, AUC: 0.6416, AP: 0.6745\n",
            "Epoch: 080, AUC: 0.6419, AP: 0.6753\n",
            "Epoch: 081, AUC: 0.6425, AP: 0.6763\n",
            "Epoch: 082, AUC: 0.6428, AP: 0.6770\n",
            "Epoch: 083, AUC: 0.6429, AP: 0.6776\n",
            "Epoch: 084, AUC: 0.6431, AP: 0.6782\n",
            "Epoch: 085, AUC: 0.6437, AP: 0.6791\n",
            "Epoch: 086, AUC: 0.6442, AP: 0.6799\n",
            "Epoch: 087, AUC: 0.6449, AP: 0.6813\n",
            "Epoch: 088, AUC: 0.6457, AP: 0.6823\n",
            "Epoch: 089, AUC: 0.6465, AP: 0.6837\n",
            "Epoch: 090, AUC: 0.6476, AP: 0.6854\n",
            "Epoch: 091, AUC: 0.6484, AP: 0.6868\n",
            "Epoch: 092, AUC: 0.6487, AP: 0.6878\n",
            "Epoch: 093, AUC: 0.6493, AP: 0.6891\n",
            "Epoch: 094, AUC: 0.6506, AP: 0.6911\n",
            "Epoch: 095, AUC: 0.6524, AP: 0.6933\n",
            "Epoch: 096, AUC: 0.6538, AP: 0.6959\n",
            "Epoch: 097, AUC: 0.6559, AP: 0.6983\n",
            "Epoch: 098, AUC: 0.6570, AP: 0.7001\n",
            "Epoch: 099, AUC: 0.6577, AP: 0.7018\n",
            "Epoch: 100, AUC: 0.6576, AP: 0.7029\n",
            "Epoch: 101, AUC: 0.6578, AP: 0.7040\n",
            "Epoch: 102, AUC: 0.6586, AP: 0.7053\n",
            "Epoch: 103, AUC: 0.6603, AP: 0.7073\n",
            "Epoch: 104, AUC: 0.6623, AP: 0.7094\n",
            "Epoch: 105, AUC: 0.6653, AP: 0.7111\n",
            "Epoch: 106, AUC: 0.6692, AP: 0.7132\n",
            "Epoch: 107, AUC: 0.6724, AP: 0.7149\n",
            "Epoch: 108, AUC: 0.6748, AP: 0.7164\n",
            "Epoch: 109, AUC: 0.6772, AP: 0.7174\n",
            "Epoch: 110, AUC: 0.6800, AP: 0.7188\n",
            "Epoch: 111, AUC: 0.6837, AP: 0.7206\n",
            "Epoch: 112, AUC: 0.6895, AP: 0.7235\n",
            "Epoch: 113, AUC: 0.6961, AP: 0.7269\n",
            "Epoch: 114, AUC: 0.7037, AP: 0.7308\n",
            "Epoch: 115, AUC: 0.7097, AP: 0.7349\n",
            "Epoch: 116, AUC: 0.7159, AP: 0.7385\n",
            "Epoch: 117, AUC: 0.7205, AP: 0.7414\n",
            "Epoch: 118, AUC: 0.7245, AP: 0.7436\n",
            "Epoch: 119, AUC: 0.7285, AP: 0.7456\n",
            "Epoch: 120, AUC: 0.7325, AP: 0.7480\n",
            "Epoch: 121, AUC: 0.7369, AP: 0.7503\n",
            "Epoch: 122, AUC: 0.7415, AP: 0.7527\n",
            "Epoch: 123, AUC: 0.7466, AP: 0.7556\n",
            "Epoch: 124, AUC: 0.7504, AP: 0.7574\n",
            "Epoch: 125, AUC: 0.7523, AP: 0.7577\n",
            "Epoch: 126, AUC: 0.7537, AP: 0.7584\n",
            "Epoch: 127, AUC: 0.7539, AP: 0.7587\n",
            "Epoch: 128, AUC: 0.7542, AP: 0.7591\n",
            "Epoch: 129, AUC: 0.7548, AP: 0.7595\n",
            "Epoch: 130, AUC: 0.7559, AP: 0.7598\n",
            "Epoch: 131, AUC: 0.7573, AP: 0.7603\n",
            "Epoch: 132, AUC: 0.7578, AP: 0.7595\n",
            "Epoch: 133, AUC: 0.7587, AP: 0.7591\n",
            "Epoch: 134, AUC: 0.7593, AP: 0.7595\n",
            "Epoch: 135, AUC: 0.7598, AP: 0.7607\n",
            "Epoch: 136, AUC: 0.7603, AP: 0.7618\n",
            "Epoch: 137, AUC: 0.7608, AP: 0.7622\n",
            "Epoch: 138, AUC: 0.7617, AP: 0.7627\n",
            "Epoch: 139, AUC: 0.7623, AP: 0.7624\n",
            "Epoch: 140, AUC: 0.7634, AP: 0.7613\n",
            "Epoch: 141, AUC: 0.7641, AP: 0.7608\n",
            "Epoch: 142, AUC: 0.7647, AP: 0.7622\n",
            "Epoch: 143, AUC: 0.7651, AP: 0.7638\n",
            "Epoch: 144, AUC: 0.7652, AP: 0.7644\n",
            "Epoch: 145, AUC: 0.7653, AP: 0.7647\n",
            "Epoch: 146, AUC: 0.7656, AP: 0.7646\n",
            "Epoch: 147, AUC: 0.7665, AP: 0.7645\n",
            "Epoch: 148, AUC: 0.7668, AP: 0.7636\n",
            "Epoch: 149, AUC: 0.7671, AP: 0.7632\n",
            "Epoch: 150, AUC: 0.7671, AP: 0.7633\n",
            "Epoch: 151, AUC: 0.7678, AP: 0.7647\n",
            "Epoch: 152, AUC: 0.7682, AP: 0.7662\n",
            "Epoch: 153, AUC: 0.7687, AP: 0.7671\n",
            "Epoch: 154, AUC: 0.7690, AP: 0.7674\n",
            "Epoch: 155, AUC: 0.7691, AP: 0.7672\n",
            "Epoch: 156, AUC: 0.7691, AP: 0.7665\n",
            "Epoch: 157, AUC: 0.7691, AP: 0.7665\n",
            "Epoch: 158, AUC: 0.7693, AP: 0.7667\n",
            "Epoch: 159, AUC: 0.7698, AP: 0.7674\n",
            "Epoch: 160, AUC: 0.7705, AP: 0.7687\n",
            "Epoch: 161, AUC: 0.7704, AP: 0.7686\n",
            "Epoch: 162, AUC: 0.7704, AP: 0.7685\n",
            "Epoch: 163, AUC: 0.7703, AP: 0.7683\n",
            "Epoch: 164, AUC: 0.7701, AP: 0.7679\n",
            "Epoch: 165, AUC: 0.7701, AP: 0.7676\n",
            "Epoch: 166, AUC: 0.7698, AP: 0.7664\n",
            "Epoch: 167, AUC: 0.7700, AP: 0.7668\n",
            "Epoch: 168, AUC: 0.7702, AP: 0.7674\n",
            "Epoch: 169, AUC: 0.7700, AP: 0.7673\n",
            "Epoch: 170, AUC: 0.7701, AP: 0.7675\n",
            "Epoch: 171, AUC: 0.7700, AP: 0.7672\n",
            "Epoch: 172, AUC: 0.7696, AP: 0.7664\n",
            "Epoch: 173, AUC: 0.7696, AP: 0.7663\n",
            "Epoch: 174, AUC: 0.7696, AP: 0.7664\n",
            "Epoch: 175, AUC: 0.7695, AP: 0.7663\n",
            "Epoch: 176, AUC: 0.7696, AP: 0.7674\n",
            "Epoch: 177, AUC: 0.7694, AP: 0.7671\n",
            "Epoch: 178, AUC: 0.7691, AP: 0.7669\n",
            "Epoch: 179, AUC: 0.7690, AP: 0.7670\n",
            "Epoch: 180, AUC: 0.7691, AP: 0.7671\n",
            "Epoch: 181, AUC: 0.7690, AP: 0.7671\n",
            "Epoch: 182, AUC: 0.7689, AP: 0.7671\n",
            "Epoch: 183, AUC: 0.7689, AP: 0.7671\n",
            "Epoch: 184, AUC: 0.7690, AP: 0.7671\n",
            "Epoch: 185, AUC: 0.7686, AP: 0.7666\n",
            "Epoch: 186, AUC: 0.7684, AP: 0.7664\n",
            "Epoch: 187, AUC: 0.7684, AP: 0.7664\n",
            "Epoch: 188, AUC: 0.7684, AP: 0.7668\n",
            "Epoch: 189, AUC: 0.7685, AP: 0.7670\n",
            "Epoch: 190, AUC: 0.7683, AP: 0.7668\n",
            "Epoch: 191, AUC: 0.7677, AP: 0.7663\n",
            "Epoch: 192, AUC: 0.7675, AP: 0.7661\n",
            "Epoch: 193, AUC: 0.7675, AP: 0.7662\n",
            "Epoch: 194, AUC: 0.7675, AP: 0.7663\n",
            "Epoch: 195, AUC: 0.7678, AP: 0.7669\n",
            "Epoch: 196, AUC: 0.7680, AP: 0.7671\n",
            "Epoch: 197, AUC: 0.7679, AP: 0.7671\n",
            "Epoch: 198, AUC: 0.7675, AP: 0.7666\n",
            "Epoch: 199, AUC: 0.7670, AP: 0.7661\n",
            "Epoch: 200, AUC: 0.7670, AP: 0.7660\n",
            "Epoch: 201, AUC: 0.7673, AP: 0.7664\n",
            "Epoch: 202, AUC: 0.7676, AP: 0.7668\n",
            "Epoch: 203, AUC: 0.7676, AP: 0.7669\n",
            "Epoch: 204, AUC: 0.7674, AP: 0.7667\n",
            "Epoch: 205, AUC: 0.7671, AP: 0.7664\n",
            "Epoch: 206, AUC: 0.7670, AP: 0.7663\n",
            "Epoch: 207, AUC: 0.7667, AP: 0.7660\n",
            "Epoch: 208, AUC: 0.7666, AP: 0.7658\n",
            "Epoch: 209, AUC: 0.7665, AP: 0.7656\n",
            "Epoch: 210, AUC: 0.7664, AP: 0.7655\n",
            "Epoch: 211, AUC: 0.7662, AP: 0.7655\n",
            "Epoch: 212, AUC: 0.7660, AP: 0.7652\n",
            "Epoch: 213, AUC: 0.7656, AP: 0.7647\n",
            "Epoch: 214, AUC: 0.7654, AP: 0.7645\n",
            "Epoch: 215, AUC: 0.7656, AP: 0.7648\n",
            "Epoch: 216, AUC: 0.7658, AP: 0.7653\n",
            "Epoch: 217, AUC: 0.7658, AP: 0.7652\n",
            "Epoch: 218, AUC: 0.7655, AP: 0.7649\n",
            "Epoch: 219, AUC: 0.7652, AP: 0.7644\n",
            "Epoch: 220, AUC: 0.7648, AP: 0.7641\n",
            "Epoch: 221, AUC: 0.7650, AP: 0.7644\n",
            "Epoch: 222, AUC: 0.7653, AP: 0.7645\n",
            "Epoch: 223, AUC: 0.7654, AP: 0.7648\n",
            "Epoch: 224, AUC: 0.7654, AP: 0.7647\n",
            "Epoch: 225, AUC: 0.7652, AP: 0.7645\n",
            "Epoch: 226, AUC: 0.7645, AP: 0.7639\n",
            "Epoch: 227, AUC: 0.7641, AP: 0.7634\n",
            "Epoch: 228, AUC: 0.7643, AP: 0.7637\n",
            "Epoch: 229, AUC: 0.7642, AP: 0.7637\n",
            "Epoch: 230, AUC: 0.7645, AP: 0.7641\n",
            "Epoch: 231, AUC: 0.7648, AP: 0.7642\n",
            "Epoch: 232, AUC: 0.7648, AP: 0.7643\n",
            "Epoch: 233, AUC: 0.7648, AP: 0.7643\n",
            "Epoch: 234, AUC: 0.7646, AP: 0.7640\n",
            "Epoch: 235, AUC: 0.7644, AP: 0.7638\n",
            "Epoch: 236, AUC: 0.7642, AP: 0.7637\n",
            "Epoch: 237, AUC: 0.7641, AP: 0.7636\n",
            "Epoch: 238, AUC: 0.7640, AP: 0.7634\n",
            "Epoch: 239, AUC: 0.7639, AP: 0.7633\n",
            "Epoch: 240, AUC: 0.7641, AP: 0.7635\n",
            "Epoch: 241, AUC: 0.7646, AP: 0.7641\n",
            "Epoch: 242, AUC: 0.7648, AP: 0.7642\n",
            "Epoch: 243, AUC: 0.7646, AP: 0.7641\n",
            "Epoch: 244, AUC: 0.7644, AP: 0.7638\n",
            "Epoch: 245, AUC: 0.7639, AP: 0.7633\n",
            "Epoch: 246, AUC: 0.7640, AP: 0.7636\n",
            "Epoch: 247, AUC: 0.7644, AP: 0.7640\n",
            "Epoch: 248, AUC: 0.7650, AP: 0.7643\n",
            "Epoch: 249, AUC: 0.7651, AP: 0.7644\n",
            "Epoch: 250, AUC: 0.7648, AP: 0.7642\n",
            "Epoch: 251, AUC: 0.7643, AP: 0.7639\n",
            "Epoch: 252, AUC: 0.7644, AP: 0.7639\n",
            "Epoch: 253, AUC: 0.7647, AP: 0.7641\n",
            "Epoch: 254, AUC: 0.7648, AP: 0.7642\n",
            "Epoch: 255, AUC: 0.7646, AP: 0.7639\n",
            "Epoch: 256, AUC: 0.7647, AP: 0.7640\n",
            "Epoch: 257, AUC: 0.7650, AP: 0.7644\n",
            "Epoch: 258, AUC: 0.7652, AP: 0.7645\n",
            "Epoch: 259, AUC: 0.7651, AP: 0.7644\n",
            "Epoch: 260, AUC: 0.7648, AP: 0.7642\n",
            "Epoch: 261, AUC: 0.7648, AP: 0.7641\n",
            "Epoch: 262, AUC: 0.7650, AP: 0.7642\n",
            "Epoch: 263, AUC: 0.7651, AP: 0.7643\n",
            "Epoch: 264, AUC: 0.7652, AP: 0.7643\n",
            "Epoch: 265, AUC: 0.7653, AP: 0.7643\n",
            "Epoch: 266, AUC: 0.7654, AP: 0.7644\n",
            "Epoch: 267, AUC: 0.7654, AP: 0.7644\n",
            "Epoch: 268, AUC: 0.7655, AP: 0.7644\n",
            "Epoch: 269, AUC: 0.7656, AP: 0.7644\n",
            "Epoch: 270, AUC: 0.7658, AP: 0.7646\n",
            "Epoch: 271, AUC: 0.7660, AP: 0.7648\n",
            "Epoch: 272, AUC: 0.7657, AP: 0.7644\n",
            "Epoch: 273, AUC: 0.7656, AP: 0.7644\n",
            "Epoch: 274, AUC: 0.7655, AP: 0.7643\n",
            "Epoch: 275, AUC: 0.7655, AP: 0.7642\n",
            "Epoch: 276, AUC: 0.7657, AP: 0.7644\n",
            "Epoch: 277, AUC: 0.7657, AP: 0.7642\n",
            "Epoch: 278, AUC: 0.7656, AP: 0.7641\n",
            "Epoch: 279, AUC: 0.7652, AP: 0.7636\n",
            "Epoch: 280, AUC: 0.7650, AP: 0.7634\n",
            "Epoch: 281, AUC: 0.7651, AP: 0.7635\n",
            "Epoch: 282, AUC: 0.7650, AP: 0.7635\n",
            "Epoch: 283, AUC: 0.7648, AP: 0.7632\n",
            "Epoch: 284, AUC: 0.7649, AP: 0.7634\n",
            "Epoch: 285, AUC: 0.7650, AP: 0.7636\n",
            "Epoch: 286, AUC: 0.7648, AP: 0.7634\n",
            "Epoch: 287, AUC: 0.7648, AP: 0.7633\n",
            "Epoch: 288, AUC: 0.7647, AP: 0.7632\n",
            "Epoch: 289, AUC: 0.7646, AP: 0.7632\n",
            "Epoch: 290, AUC: 0.7647, AP: 0.7634\n",
            "Epoch: 291, AUC: 0.7644, AP: 0.7631\n",
            "Epoch: 292, AUC: 0.7645, AP: 0.7634\n",
            "Epoch: 293, AUC: 0.7643, AP: 0.7631\n",
            "Epoch: 294, AUC: 0.7641, AP: 0.7629\n",
            "Epoch: 295, AUC: 0.7646, AP: 0.7636\n",
            "Epoch: 296, AUC: 0.7648, AP: 0.7639\n",
            "Epoch: 297, AUC: 0.7649, AP: 0.7640\n",
            "Epoch: 298, AUC: 0.7649, AP: 0.7640\n",
            "Epoch: 299, AUC: 0.7641, AP: 0.7633\n",
            "Epoch: 300, AUC: 0.7641, AP: 0.7633\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "enKmV9nKjnp9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}